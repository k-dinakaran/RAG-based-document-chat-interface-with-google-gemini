{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMH+5dwszll9takxmZmgzcn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k-dinakaran/RAG-based-document-chat-interface-with-google-gemini/blob/main/RAG_based_document_chat_interface_with_google_gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First I install all required dependencies\n",
        "!pip install -q gradio langchain langchain-community langchain-google-genai sentence-transformers faiss-cpu pypdf unstructured docx2txt tqdm python-dotenv\n",
        "\n",
        "# Import required libraries\n",
        "from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader, TextLoader, CSVLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from typing import TypedDict, Annotated, List\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "import operator\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# here i Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"chunk_size\": 1000,\n",
        "    \"chunk_overlap\": 200,\n",
        "    \"max_retrieved_docs\": 3,\n",
        "    \"max_context_length\": 3000,\n",
        "    \"max_response_length\": 500,\n",
        "    \"max_file_size_mb\": 10,\n",
        "    \"chat_history_length\": 6,\n",
        "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"llm_model\": \"gemini-pro\",\n",
        "    \"llm_temperature\": 0.3\n",
        "}\n",
        "\n",
        "# here i given my gemini api key\n",
        "GEMINI_API_KEY = \"AIzaSyAjJFwdpqB2inMa1hhtU4huvf__WgeFoOI\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# here i initialize LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
        "\n",
        "# there are the function for Document processing\n",
        "def validate_file(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        raise ValueError(\"File does not exist\")\n",
        "    file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
        "    if file_size > CONFIG[\"max_file_size_mb\"]:\n",
        "        raise ValueError(f\"File too large (max {CONFIG['max_file_size_mb']}MB)\")\n",
        "\n",
        "def load_documents(file_path):\n",
        "    try:\n",
        "        validate_file(file_path)\n",
        "        if file_path.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif file_path.endswith('.docx'):\n",
        "            loader = UnstructuredWordDocumentLoader(file_path)\n",
        "        elif file_path.endswith('.txt'):\n",
        "            loader = TextLoader(file_path)\n",
        "        elif file_path.endswith('.csv'):\n",
        "            loader = CSVLoader(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {os.path.splitext(file_path)[1]}\")\n",
        "\n",
        "        pages = []\n",
        "        for page in loader.lazy_load():\n",
        "            pages.append(page)\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=CONFIG[\"chunk_size\"],\n",
        "            chunk_overlap=CONFIG[\"chunk_overlap\"]\n",
        "        )\n",
        "        return text_splitter.split_documents(pages)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading document: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "#here i used FAISS based Vector store for management\n",
        "vector_store = None\n",
        "retriever = None\n",
        "last_processed_file = None\n",
        "\n",
        "def create_vector_store(docs):\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=CONFIG[\"embedding_model\"],\n",
        "        model_kwargs={'device': 'cpu'}\n",
        "    )\n",
        "    return FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "def process_document(file):\n",
        "    global vector_store, retriever, last_processed_file\n",
        "    if not file:\n",
        "        return \"Please upload a document first.\"\n",
        "    if last_processed_file == file.name:\n",
        "        return \"Document already loaded.\"\n",
        "\n",
        "    try:\n",
        "        documents = load_documents(file.name)\n",
        "        vector_store = create_vector_store(documents)\n",
        "        retriever = vector_store.as_retriever()\n",
        "        last_processed_file = file.name\n",
        "        return \"Document loaded successfully!\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "#function for  Chat\n",
        "def chat_with_docs(message, history):\n",
        "    if not retriever:\n",
        "        return history + [[message, \"Please load a document first.\"]]\n",
        "\n",
        "    try:\n",
        "        docs = retriever.get_relevant_documents(message)\n",
        "        response = f\"I found {len(docs)} relevant sections:\\n\\n\" + \"\\n---\\n\".join(\n",
        "            [d.page_content[:300] for d in docs[:3]])  # Show first 3 snippets\n",
        "        return history + [[message, response]]\n",
        "    except Exception as e:\n",
        "        return history + [[message, f\"Error: {str(e)}\"]]\n",
        "\n",
        "#implemented Gradio interface for uploading document and chatting\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Document Chat with RAG\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            file_input = gr.File(label=\"Upload Document\",\n",
        "                               file_types=[\".pdf\", \".docx\", \".txt\", \".csv\"])\n",
        "            load_btn = gr.Button(\"Load Document\")\n",
        "            load_status = gr.Textbox(label=\"Status\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot()\n",
        "            msg = gr.Textbox(label=\"Your Question\")\n",
        "            submit_btn = gr.Button(\"Submit\")\n",
        "            clear_btn = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    load_btn.click(process_document, file_input, load_status)\n",
        "    submit_btn.click(chat_with_docs, [msg, chatbot], [chatbot]).then(lambda: \"\", None, msg)\n",
        "    msg.submit(chat_with_docs, [msg, chatbot], [chatbot]).then(lambda: \"\", None, msg)\n",
        "    clear_btn.click(clear_chat, None, chatbot)\n",
        "\n",
        "# Launch the app to run my model\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "_imo8M7nc2EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hVxl-RfLdShv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}